# Modelos de probabilidad

Hasta ahora hemos trabajado con regresiones en las que la variable dependiente es continua: ventas, gasto promedio, calificaciones de clientes. Pero en la pr√°ctica, muchas decisiones importantes se expresan como un **s√≠ o no**:

- ¬øEl cliente regresar√° al restaurante la pr√≥xima semana?  
- ¬øEl nuevo plato es un √©xito (se vende) o un fracaso (no se vende)?  
- ¬øEl cliente recomienda el restaurante a un amigo o no?  

Estas situaciones se modelan con **variables binarias**: toman el valor 1 cuando el evento ocurre y 0 cuando no.

En este cap√≠tulo introducimos modelos donde la variable respuesta es una **probabilidad** o una **categor√≠a**, en lugar de una cantidad continua.

## El primer intento: Modelo de Probabilidad Lineal (MPL)

Podemos intentar usar regresi√≥n lineal directamente, tratando la variable binaria como si fuera continua. El modelo se ver√≠a as√≠:

$$
Pr(\text{Regresa}=1) = b_0 + b_1 \times Precio + b_2 \times Publicidad + \varepsilon
$$

Aqu√≠:  

- $b_1$ indica c√≥mo cambia la **probabilidad** de regreso cuando aumenta el precio en 1 unidad.  
- $b_2$ mide el efecto de invertir m√°s en publicidad.  

**Ejemplo en el restaurante:**  
- Cada $1 adicional en el precio del men√∫ disminuye la probabilidad de regreso en 3 puntos porcentuales.  
- Cada $100 en publicidad aumenta la probabilidad en 2 puntos porcentuales.  

::: callout-warning
**Problema:** este modelo puede predecir probabilidades menores a 0 o mayores a 1, lo cual no tiene sentido.
:::


## La soluci√≥n: Regresi√≥n Log√≠stica

Para asegurarnos de que las probabilidades siempre est√©n entre 0 y 1, usamos una transformaci√≥n llamada **funci√≥n log√≠stica**:

$$
Pr(\text{Regresa}=1) = \frac{1}{1 + e^{-(b_0 + b_1 \times Precio + b_2 \times Publicidad)}}
$$

- Sin importar los valores de Precio o Publicidad, el resultado siempre es una probabilidad v√°lida.  
- La relaci√≥n entre las variables y la probabilidad no es lineal, sino en forma de ‚ÄúS‚Äù: los cambios tienen m√°s impacto en ciertos rangos y menos en otros.


## Intuici√≥n con el restaurante üçΩÔ∏è

Supongamos que estimamos el siguiente modelo:

$$
Pr(\text{Regresa}=1) = \frac{1}{1 + e^{-(2 - 0.05 \times Precio + 0.01 \times Publicidad)}}
$$

Interpretaci√≥n:  
- Cada $1 m√°s en el precio reduce la probabilidad de regreso en aproximadamente 5%.  
- Cada $100 invertidos en publicidad aumentan la probabilidad en 1%.  

Ejemplos:  
- Men√∫ a $20 y publicidad de $500 ‚Üí probabilidad de regreso = 70%.  
- Men√∫ a $30 y misma publicidad ‚Üí probabilidad de regreso = 55%.  

```{r}
#| label: ejemplo-logit
#| fig-cap: "Curva log√≠stica: probabilidad de que un cliente regrese seg√∫n el precio"
#| echo: false

precio <- seq(10,40,1)
prob <- 1/(1+exp(-(2 -0.05*precio + 0.01*500)))
plot(precio, prob, type="l", lwd=2, col="darkblue",
     xlab="Precio del men√∫", ylab="Probabilidad de regreso")
```

## ¬øCu√°ndo usar el Modelo de Probabilidad Lineal (MPL) y cu√°ndo la Regresi√≥n Log√≠stica?

Breve respuesta intuitiva:

- Usa el MPL para exploraciones r√°pidas o cuando quieras una lectura lineal sencilla de los datos. Es f√°cil de estimar y explicar. Sirve mejor para efectos generales.
- Usa la regresi√≥n log√≠stica cuando necesites predicciones de probabilidad coherentes (siempre entre 0 y 1) para cada individuo, o cuando la relaci√≥n entre las variables y la probabilidad tenga curvatura (forma de "S").

M√°s en detalle:

- ¬øPor qu√© el MPL puede funcionar como primer paso?
     - Es un modelo lineal simple: el coeficiente se interpreta directamente como ‚Äúcambio en la probabilidad (puntos porcentuales) por unidad‚Äù.
     - Es √∫til para explorar se√±ales en los datos y para comunicaci√≥n r√°pida cuando las predicciones quedan mayoritariamente dentro de 0‚Äì1.

- ¬øPor qu√© puede fallar el MPL?
     - Puede predecir probabilidades menores que 0 o mayores que 1.
     - No captura la naturaleza no lineal t√≠pica de muchas probabilidades (efectos que se saturan).
     - Sus errores est√°ndar pueden ser incorrectos por la heterocedasticidad inherente a variables binarias.

- ¬øPor qu√© la regresi√≥n log√≠stica suele ser mejor?
     - La transformaci√≥n log√≠stica garantiza probabilidades en [0,1].
     - Modela la curvatura (efectos m√°s fuertes en ciertas zonas y m√°s d√©biles en los extremos).
     - Es el est√°ndar pr√°ctico cuando se requiere rigor en predicci√≥n y en inferencia para variables binarias.

*Regla pr√°ctica:*

- Si necesitas rapidez y las probabilidades estimadas caen dentro de 0.1‚Äì0.9 sin comportamientos extra√±os, el MPL puede servir para explorar. Pero si necesitas predicciones confiables, interpretaci√≥n robusta y valores fuera del rango son posibles, usa log√≠stica.

## Interpretaci√≥n: coeficientes, log-odds y efectos marginales (explicado f√°cil)

Lo esencial de forma muy simple:

- En regresi√≥n lineal un coeficiente $Œ≤_j$ se lee directo: un aumento de 1 en $X_j$ cambia la probabilidad en $Œ≤_j$ puntos porcentuales.
- En regresi√≥n log√≠stica NO se lee as√≠. Los coeficientes act√∫an sobre los *log-odds* (el logaritmo de las odds), no sobre la probabilidad directa.

¬øQu√© quiere decir eso en palabras?
- Piensa que la regresi√≥n log√≠stica primero calcula una "puntuaci√≥n" $(x \times Œ≤)$. Esa puntuaci√≥n se transforma con la funci√≥n log√≠stica para producir una probabilidad entre 0 y 1. El coeficiente $Œ≤_j$ dice cu√°nto cambia la puntuaci√≥n (el log-odds), pero ese cambio no se traduce siempre en la misma variaci√≥n de probabilidad: depende del punto donde est√©s.

Efectos marginales ‚Äî la forma pr√°ctica de leer resultados:

- Para saber cu√°nto cambia la probabilidad cuando $X_j$ aumenta en 1 unidad necesitamos calcular el *efecto marginal*, que combina $Œ≤_j$ con la probabilidad actual p:

     cambio en probabilidad ‚âà $$ Œ≤_j √ó p √ó (1 - p)$$

     - Observa que p(1-p) es m√°ximo cuando p ‚âà 0.5, y peque√±o cuando p est√° cerca de 0 o 1. Esto significa que un mismo $Œ≤_j$ puede implicar cambios grandes en la probabilidad si estamos en la zona media, y cambios peque√±os si estamos en los extremos.

Ejemplo muy concreto:
- Si $Œ≤_precio = -0.05$ y para un cliente $p = 0.5$, el efecto aproximado de subir 1 unidad el precio es:

$-0.05 √ó 0.5 √ó 0.5 = -0.0125$ ‚Üí una reducci√≥n de $~1.25$ puntos porcentuales en la probabilidad de regreso en ese punto.

C√≥mo reportarlo en pr√°ctica:

- Efecto marginal en la media (MEM): calcula $p$ en las medias de las covariables y aplica la f√≥rmula. F√°cil de explicar.
- Efecto marginal promedio (AME): calcula el efecto marginal por observaci√≥n y promedia; refleja el efecto medio en la muestra.
- Cambio discreto para variables dummy: calcula la diferencia en la probabilidad predicha al pasar la variable de 0 a 1.

Otra forma com√∫n: odds y odds ratio (explicado f√°cil)

Cuando se estima un modelo binomial (log√≠stico) los coeficientes se obtienen en la escala de los *log-odds*. Para pasar a una interpretaci√≥n m√°s intuitiva aplicamos la exponencial de ese coeficiente:

- OR = $exp(Œ≤_j)$

Esto se denomina *odds ratio* (OR) y compara c√≥mo cambian las odds cuando una variable aumenta en una unidad.

Definiciones claras e intuitivas:

- Odds: la raz√≥n entre la probabilidad de √©xito y la probabilidad de fracaso.

     odds = $p / (1 - p)$

     donde $p$ es la probabilidad de que ocurra el evento de inter√©s.

- Odds ratio (OR): la raz√≥n entre las odds en dos grupos o condiciones.

     OR = odds en el grupo A / odds en el grupo B

     Si OR > 1, las odds son mayores en el numerador; si OR < 1, las odds son menores.

Ejemplo simple e intuitivo:

- Si $exp(Œ≤_precio) = 0.95$, eso significa que aumentar el precio en 1 unidad multiplica las odds por $0.95$ (una reducci√≥n del 5% en las odds). No es exactamente la misma reducci√≥n en la probabilidad, pero da una idea clara de direcci√≥n y magnitud relativa.

Consejo pr√°ctico para comunicar resultados:

- Usa OR cuando quieras mostrar ¬´cu√°nto cambian las odds¬ª (√∫til en reportes) y complementa siempre con efectos marginales o cambios en probabilidades para audiencias no t√©cnicas.

Resumen para el lector final:

- MPL: simple y √∫til para exploraci√≥n, pero con l√≠mites (probabilidades fuera de rango, errores en inferencia).
- Log√≠stica: un poco m√°s compleja de interpretar, pero produce probabilidades coherentes; siempre traduzcas Œ≤ a efectos marginales o a cambios en probabilidad para comunicar resultados a audiencias no t√©cnicas.


